{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Katakana seq2seq model (credit to wanasit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a .csv file, containing english words on the first column and their translation in katakana in the second column (built by wanasit, see https://wanasit.github.io/english-to-katakana-using-sequence-to-sequence-in-keras.html for more details). We're going to build a model that automatically does the conversion from english to katakana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0                1\n",
      "76132           Chewbacca           チューバッカ\n",
      "88968         Marin Bikes         マリン・バイクス\n",
      "12942             Greyout           グレイアウト\n",
      "58630   Georges Rodenbach    ジョルジュ・ローデンバック\n",
      "20202        Mike Maignan         マイク・メニャン\n",
      "44311    Jacob Ellehammer        ヤコブ・エレハマー\n",
      "90928        Samurai Jack         サムライジャック\n",
      "23014          Jony López  ジョナタン・ロペス・ロドリゲス\n",
      "44733          Ben Revere           ベン・リビア\n",
      "63059       Streptokinase        ストレプトキナーゼ\n",
      "16678              Ossian             オシアン\n",
      "92801   Gouverneur Morris        ガバヌーア・モリス\n",
      "61466           Stralsund        シュトラールズント\n",
      "102620   Friedemann Layer      フリーデマン・レイヤー\n",
      "32974      Dorothy Malone        ドロシー・マローン\n",
      "43889           Sally Yeh          サリー・イップ\n",
      "88822            Parndorf           パルンドルフ\n",
      "60547            Landshut           ランツフート\n",
      "86368      Heike Makatsch         ハイケ・マカチュ\n",
      "12315               Evraz        エブラズ・グループ\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./trainingdata/joined_titles.csv', header=None)\n",
    "print(data.sample(20, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this data into a X and Y vectors for training first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [word.lower() for word in data[0]]\n",
    "Y = [word for word in data[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not done yet. Our model only takes numerical data and we cannot input strings directly. We have to build a sort of encoding for each character in english and in katakana, but also store the way to decode the characters so we can read the output of the model at the end. Also remember that a model only takes input with same size, so we have to use padding. Let's use 0 for padding, 1 for start of sequence, and just code all the characters as int based on the order they appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOW_token = 1\n",
    "\n",
    "\n",
    "class CharEncoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {} #the index will be our encoding of the char\n",
    "        self.char2count = {}\n",
    "        self.index2char = {1: \"SOS\"}\n",
    "        self.n_chars = 2  # Count SOS \n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishEncoder = CharEncoder(\"english\")\n",
    "katakanaEncoder = CharEncoder(\"katakana\")\n",
    "for word in X:\n",
    "    englishEncoder.addWord(word)\n",
    "for word in Y:\n",
    "    katakanaEncoder.addWord(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ア': 2, 'ン': 3, 'ス': 4, 'ク': 5, 'ー': 6, 'リ': 7, 'グ': 8, 'ロ': 9, 'ヴ': 10, 'ォ': 11, 'シ': 12, 'ツ': 13, 'ェ': 14, 'ミ': 15, 'ル': 16, 'ヒ': 17, 'ユ': 18, 'ァ': 19, 'ブ': 20, 'レ': 21, 'ビ': 22, 'ッ': 23, 'ラ': 24, 'サ': 25, 'パ': 26, 'マ': 27, 'ノ': 28, 'ザ': 29, 'ポ': 30, 'ト': 31, 'デ': 32, 'フ': 33, 'テ': 34, 'エ': 35, 'ヘ': 36, 'タ': 37, 'ィ': 38, 'バ': 39, 'キ': 40, 'ペ': 41, 'ソ': 42, 'ナ': 43, 'イ': 44, 'ゼ': 45, 'ョ': 46, 'ダ': 47, 'ゴ': 48, 'ボ': 49, 'カ': 50, 'ガ': 51, 'ハ': 52, 'ベ': 53, 'コ': 54, '・': 55, 'ヌ': 56, 'オ': 57, 'ネ': 58, 'ド': 59, 'ズ': 60, 'モ': 61, 'ウ': 62, 'ジ': 63, 'ニ': 64, 'ュ': 65, 'メ': 66, 'ゲ': 67, 'ャ': 68, 'ピ': 69, 'プ': 70, 'セ': 71, 'ゾ': 72, 'チ': 73, 'ヨ': 74, 'ギ': 75, 'ヤ': 76, 'ホ': 77, 'ム': 78, 'ゥ': 79, 'ワ': 80, 'ケ': 81, ' ': 82, 'ヅ': 83, 'ヂ': 84, 'ヱ': 85, 'ヮ': 86, 'ヰ': 87, 'ヲ': 88}\n",
      "{1: 'SOS', 2: 'u', 3: 'n', 4: 's', 5: 'c', 6: 'h', 7: 'o', 8: 'l', 9: 'i', 10: 'g', 11: 'v', 12: 'e', 13: 'm', 14: 'j', 15: 'a', 16: 'b', 17: 'r', 18: ' ', 19: 'p', 20: 't', 21: 'd', 22: 'k', 23: 'y', 24: 'z', 25: 'f', 26: 'ü', 27: 'w', 28: 'x', 29: 'q', 30: 'ú', 31: 'ž', 32: 'ý', 33: 'ó', 34: 'ù', 35: 'ò', 36: '2', 37: '5', 38: '0', 39: 'ê', 40: 'ż', 41: 'õ', 42: 'þ', 43: '3', 44: 'ľ', 45: '4', 46: 'ļ', 47: 'ź', 48: '6', 49: '1', 50: '7', 51: 'ŵ', 52: '9', 53: '8'}\n"
     ]
    }
   ],
   "source": [
    "print(katakanaEncoder.char2index)\n",
    "print(englishEncoder.index2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have succesfully created our encoding based on the input data. We now have to convert the data using the encoding we created. We also have to pad both the input and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(encoder, word, word_length):\n",
    "    transformed_word = [0]*word_length\n",
    "    for i in range(len(word)):\n",
    "        transformed_word[i] = encoder.char2index[word[i]]\n",
    "    return transformed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = max(len(word) for word in X)\n",
    "OUTPUT_LENGTH = max(len(word) for word in Y)\n",
    "X = [indexesFromWord(englishEncoder, word, INPUT_LENGTH) for word in X]\n",
    "Y = [indexesFromWord(katakanaEncoder, word, OUTPUT_LENGTH) for word in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 7, 8, 9, 3, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 7, 11, 7, 4, 9, 5, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building our model. We first need to turn our input vector to a dense vector. For this, we have to use and Embedding layer. We then add one or more LSTM layers. These layers will serve as our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "encoder = Embedding(englishEncoder.n_chars, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(64, return_sequences=False)(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder model will generate a sequence of Katakanas. Every generated character will be used as the input of the decoder to generate the next one.\n",
    "The input will be passed to an Embedding layer to transform the input into dense vectors , just as we did for the encoder.\n",
    "We also need the encoder output to initialize the decoder.\n",
    "The final layer will be (time distributed) Dense layer that will produce the softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n",
    "decoder = Embedding(katakanaEncoder.n_chars, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(64, return_sequences=True)(decoder, initial_state=[encoder, encoder])\n",
    "decoder = TimeDistributed(Dense(katakanaEncoder.n_chars, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do in order to use this decoder model is to have a one-hot encoded structure for the softmax prediction. Also, we need to add the start sequence char at the beginning of every word in order to have our first input character for the decoder. We will also use crossvalidation to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Input Train\n",
    "encoder_input_train = X_train\n",
    "\n",
    "# Decoder Input Train\n",
    "decoder_input_train = np.zeros_like(Y_train)\n",
    "decoder_input_train[:][1:] = Y_train[:][:-1]\n",
    "decoder_input_train[:][0] = SOW_token\n",
    "\n",
    "# Decoder OutputTrain\n",
    "decoder_output_train = np.eye(katakanaEncoder.n_chars)[Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Input Validation\n",
    "encoder_input_validation = X_val\n",
    "\n",
    "# Decoder Input Validation\n",
    "decoder_input_validation = np.zeros_like(Y_val)\n",
    "decoder_input_validation[:][1:] = Y_val[:][:-1]\n",
    "decoder_input_validation[:][0] = SOW_token\n",
    "\n",
    "# Decoder Output Validation\n",
    "decoder_output_validation = np.eye(katakanaEncoder.n_chars)[Y_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! We can now train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96534 samples, validate on 10727 samples\n",
      "Epoch 1/60\n",
      "96534/96534 [==============================] - 235s 2ms/step - loss: 3.2384 - val_loss: 3.0999\n",
      "Epoch 2/60\n",
      "96534/96534 [==============================] - 218s 2ms/step - loss: 3.0131 - val_loss: 2.8804\n",
      "Epoch 3/60\n",
      "96534/96534 [==============================] - 216s 2ms/step - loss: 2.7341 - val_loss: 2.5795\n",
      "Epoch 4/60\n",
      "96534/96534 [==============================] - 217s 2ms/step - loss: 2.4602 - val_loss: 2.3428\n",
      "Epoch 5/60\n",
      "96534/96534 [==============================] - 230s 2ms/step - loss: 2.2641 - val_loss: 2.1779\n",
      "Epoch 6/60\n",
      "96534/96534 [==============================] - 221s 2ms/step - loss: 2.1219 - val_loss: 2.0656\n",
      "Epoch 7/60\n",
      "96534/96534 [==============================] - 218s 2ms/step - loss: 2.0168 - val_loss: 1.9667\n",
      "Epoch 8/60\n",
      "96534/96534 [==============================] - 212s 2ms/step - loss: 1.9368 - val_loss: 1.9068\n",
      "Epoch 9/60\n",
      "96534/96534 [==============================] - 212s 2ms/step - loss: 1.8724 - val_loss: 1.8674\n",
      "Epoch 10/60\n",
      "96534/96534 [==============================] - 211s 2ms/step - loss: 1.8200 - val_loss: 1.7978\n",
      "Epoch 11/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.7760 - val_loss: 1.7642\n",
      "Epoch 12/60\n",
      "96534/96534 [==============================] - 209s 2ms/step - loss: 1.7391 - val_loss: 1.7277\n",
      "Epoch 13/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.7066 - val_loss: 1.7035\n",
      "Epoch 14/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.6794 - val_loss: 1.6816\n",
      "Epoch 15/60\n",
      "96534/96534 [==============================] - 212s 2ms/step - loss: 1.6562 - val_loss: 1.6609\n",
      "Epoch 16/60\n",
      "96534/96534 [==============================] - 213s 2ms/step - loss: 1.6356 - val_loss: 1.6416\n",
      "Epoch 17/60\n",
      "96534/96534 [==============================] - 211s 2ms/step - loss: 1.6171 - val_loss: 1.6313\n",
      "Epoch 18/60\n",
      "96534/96534 [==============================] - 212s 2ms/step - loss: 1.6003 - val_loss: 1.6200\n",
      "Epoch 19/60\n",
      "96534/96534 [==============================] - 211s 2ms/step - loss: 1.5868 - val_loss: 1.6038\n",
      "Epoch 20/60\n",
      "96534/96534 [==============================] - 209s 2ms/step - loss: 1.5725 - val_loss: 1.5897\n",
      "Epoch 21/60\n",
      "96534/96534 [==============================] - 209s 2ms/step - loss: 1.5592 - val_loss: 1.5870\n",
      "Epoch 22/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.5497 - val_loss: 1.5747\n",
      "Epoch 23/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.5372 - val_loss: 1.5629\n",
      "Epoch 24/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.5280 - val_loss: 1.5591\n",
      "Epoch 25/60\n",
      "96534/96534 [==============================] - 210s 2ms/step - loss: 1.5191 - val_loss: 1.5462\n",
      "Epoch 26/60\n",
      "96534/96534 [==============================] - 211s 2ms/step - loss: 1.5095 - val_loss: 1.5379\n",
      "Epoch 27/60\n",
      "96534/96534 [==============================] - 213s 2ms/step - loss: 1.5019 - val_loss: 1.5446\n",
      "Epoch 28/60\n",
      "96534/96534 [==============================] - 213s 2ms/step - loss: 1.4936 - val_loss: 1.5221\n",
      "Epoch 29/60\n",
      "96534/96534 [==============================] - 212s 2ms/step - loss: 1.4858 - val_loss: 1.5209\n",
      "Epoch 30/60\n",
      "96534/96534 [==============================] - 213s 2ms/step - loss: 1.4793 - val_loss: 1.5165\n",
      "Epoch 31/60\n",
      "96534/96534 [==============================] - 216s 2ms/step - loss: 1.4727 - val_loss: 1.5070\n",
      "Epoch 32/60\n",
      "96534/96534 [==============================] - 218s 2ms/step - loss: 1.4665 - val_loss: 1.4979\n",
      "Epoch 33/60\n",
      "96534/96534 [==============================] - 220s 2ms/step - loss: 1.4603 - val_loss: 1.4943\n",
      "Epoch 34/60\n",
      "96534/96534 [==============================] - 222s 2ms/step - loss: 1.4543 - val_loss: 1.4933\n",
      "Epoch 35/60\n",
      "96534/96534 [==============================] - 223s 2ms/step - loss: 1.4490 - val_loss: 1.4889\n",
      "Epoch 36/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.4445 - val_loss: 1.4875\n",
      "Epoch 37/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.4390 - val_loss: 1.4839\n",
      "Epoch 38/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.4338 - val_loss: 1.4781\n",
      "Epoch 39/60\n",
      "96534/96534 [==============================] - 223s 2ms/step - loss: 1.4307 - val_loss: 1.4865\n",
      "Epoch 40/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.4251 - val_loss: 1.4738\n",
      "Epoch 41/60\n",
      "96534/96534 [==============================] - 225s 2ms/step - loss: 1.4216 - val_loss: 1.4754\n",
      "Epoch 42/60\n",
      "96534/96534 [==============================] - 227s 2ms/step - loss: 1.4174 - val_loss: 1.4638\n",
      "Epoch 43/60\n",
      "96534/96534 [==============================] - 235s 2ms/step - loss: 1.4135 - val_loss: 1.4703\n",
      "Epoch 44/60\n",
      "96534/96534 [==============================] - 227s 2ms/step - loss: 1.4110 - val_loss: 1.4609\n",
      "Epoch 45/60\n",
      "96534/96534 [==============================] - 226s 2ms/step - loss: 1.4083 - val_loss: 1.4595\n",
      "Epoch 46/60\n",
      "96534/96534 [==============================] - 225s 2ms/step - loss: 1.4028 - val_loss: 1.4592\n",
      "Epoch 47/60\n",
      "96534/96534 [==============================] - 225s 2ms/step - loss: 1.4019 - val_loss: 1.4717\n",
      "Epoch 48/60\n",
      "96534/96534 [==============================] - 225s 2ms/step - loss: 1.3986 - val_loss: 1.4546\n",
      "Epoch 49/60\n",
      "96534/96534 [==============================] - 227s 2ms/step - loss: 1.3947 - val_loss: 1.4450\n",
      "Epoch 50/60\n",
      "96534/96534 [==============================] - 229s 2ms/step - loss: 1.3906 - val_loss: 1.4493\n",
      "Epoch 51/60\n",
      "96534/96534 [==============================] - 226s 2ms/step - loss: 1.3891 - val_loss: 1.4449\n",
      "Epoch 52/60\n",
      "96534/96534 [==============================] - 226s 2ms/step - loss: 1.3859 - val_loss: 1.4460\n",
      "Epoch 53/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.3834 - val_loss: 1.4369\n",
      "Epoch 54/60\n",
      "96534/96534 [==============================] - 225s 2ms/step - loss: 1.3810 - val_loss: 1.4399\n",
      "Epoch 55/60\n",
      "96534/96534 [==============================] - 223s 2ms/step - loss: 1.3784 - val_loss: 1.4388\n",
      "Epoch 56/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.3757 - val_loss: 1.4363\n",
      "Epoch 57/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.3737 - val_loss: 1.4380\n",
      "Epoch 58/60\n",
      "96534/96534 [==============================] - 224s 2ms/step - loss: 1.3720 - val_loss: 1.4445\n",
      "Epoch 59/60\n",
      "96534/96534 [==============================] - 227s 2ms/step - loss: 1.3701 - val_loss: 1.4401\n",
      "Epoch 60/60\n",
      "96534/96534 [==============================] - 229s 2ms/step - loss: 1.3670 - val_loss: 1.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aimen\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/TensorArrayReadV3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "if os.path.isfile('model.h5'):\n",
    "    model = load_model('model.h5')\n",
    "else:\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit(x=[encoder_input_train, decoder_input_train], y=[decoder_output_train],\n",
    "              validation_data=([encoder_input_validation, decoder_input_validation], [decoder_output_validation]),\n",
    "                                epochs = 60,\n",
    "                                 batch_size = 128)\n",
    "   \n",
    "    \n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(word):\n",
    "    encoder_input = [indexesFromWord(englishEncoder, word.lower(), INPUT_LENGTH)]\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = SOW_token \n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return decoder_input[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(decoding, sequence):\n",
    "    text = ''\n",
    "    for i in sequence:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += decoding[i]\n",
    "    return text\n",
    "\n",
    "def to_katakana(text):\n",
    "    decoder_output = generate(text)\n",
    "    return decode(katakanaEncoder.index2char, decoder_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'エイメン'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_katakana(\"aimen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'マイューデ'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_katakana(\"mathilde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
