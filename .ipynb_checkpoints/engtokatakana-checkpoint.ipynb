{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Katakana seq2seq model (credit to wanasit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a .csv file, containing english words on the first column and their translation in katakana in the second column (built by wanasit, see https://wanasit.github.io/english-to-katakana-using-sequence-to-sequence-in-keras.html for more details). We're going to build a model that automatically does the conversion from english to katakana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-cbd2c3a21a89>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-cbd2c3a21a89>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(data.sample(20, random_seed=1)\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./trainingdata/joined_titles.csv', header=None)\n",
    "print(data.sample(20, random_seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this data into a X and Y vectors for training first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [word.lower() for word in data[0]]\n",
    "Y = [word for word in data[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not done yet. Our model only takes numerical data and we cannot input strings directly. We have to build a sort of encoding for each character in english and in katakana, but also store the way to decode the characters so we can read the output of the model at the end. Also remember that a model only takes input with same size, so we have to use padding. Let's use 0 for padding, 1 for start of sequence, and just code all the characters as int based on the order they appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_CHAR_CODE = 1\n",
    "\n",
    "def encode_characters(words):\n",
    "    count = 2\n",
    "    encoding = {}\n",
    "    decoding = {1: 'START'}\n",
    "    for c in set([char for word in words for char in word]): #gets all the chars of the data set\n",
    "        encoding[c] = count\n",
    "        decoding[count] = c\n",
    "        count += 1\n",
    "    return encoding, decoding, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
