{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Katakana seq2seq model (credit to wanasit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a .csv file, containing english words on the first column and their translation in katakana in the second column (built by wanasit, see https://wanasit.github.io/english-to-katakana-using-sequence-to-sequence-in-keras.html for more details). We're going to build a model that automatically does the conversion from english to katakana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0                1\n",
      "76132           Chewbacca           チューバッカ\n",
      "88968         Marin Bikes         マリン・バイクス\n",
      "12942             Greyout           グレイアウト\n",
      "58630   Georges Rodenbach    ジョルジュ・ローデンバック\n",
      "20202        Mike Maignan         マイク・メニャン\n",
      "44311    Jacob Ellehammer        ヤコブ・エレハマー\n",
      "90928        Samurai Jack         サムライジャック\n",
      "23014          Jony López  ジョナタン・ロペス・ロドリゲス\n",
      "44733          Ben Revere           ベン・リビア\n",
      "63059       Streptokinase        ストレプトキナーゼ\n",
      "16678              Ossian             オシアン\n",
      "92801   Gouverneur Morris        ガバヌーア・モリス\n",
      "61466           Stralsund        シュトラールズント\n",
      "102620   Friedemann Layer      フリーデマン・レイヤー\n",
      "32974      Dorothy Malone        ドロシー・マローン\n",
      "43889           Sally Yeh          サリー・イップ\n",
      "88822            Parndorf           パルンドルフ\n",
      "60547            Landshut           ランツフート\n",
      "86368      Heike Makatsch         ハイケ・マカチュ\n",
      "12315               Evraz        エブラズ・グループ\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./trainingdata/joined_titles.csv', header=None)\n",
    "print(data.sample(20, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this data into a X and Y vectors for training first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [word.lower() for word in data[0]]\n",
    "Y = [word for word in data[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not done yet. Our model only takes numerical data and we cannot input strings directly. We have to build a sort of encoding for each character in english and in katakana, but also store the way to decode the characters so we can read the output of the model at the end. Also remember that a model only takes input with same size, so we have to use padding. Let's use 0 for padding, 1 for start of sequence, and just code all the characters as int based on the order they appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOW_token = 1\n",
    "\n",
    "\n",
    "class CharEncoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {} #the index will be our encoding of the char\n",
    "        self.char2count = {}\n",
    "        self.index2char = {1: \"SOS\"}\n",
    "        self.n_chars = 2  # Count SOS \n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishEncoder = CharEncoder(\"english\")\n",
    "katakanaEncoder = CharEncoder(\"katakana\")\n",
    "for word in X:\n",
    "    englishEncoder.addWord(word)\n",
    "for word in Y:\n",
    "    katakanaEncoder.addWord(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ア': 2, 'ン': 3, 'ス': 4, 'ク': 5, 'ー': 6, 'リ': 7, 'グ': 8, 'ロ': 9, 'ヴ': 10, 'ォ': 11, 'シ': 12, 'ツ': 13, 'ェ': 14, 'ミ': 15, 'ル': 16, 'ヒ': 17, 'ユ': 18, 'ァ': 19, 'ブ': 20, 'レ': 21, 'ビ': 22, 'ッ': 23, 'ラ': 24, 'サ': 25, 'パ': 26, 'マ': 27, 'ノ': 28, 'ザ': 29, 'ポ': 30, 'ト': 31, 'デ': 32, 'フ': 33, 'テ': 34, 'エ': 35, 'ヘ': 36, 'タ': 37, 'ィ': 38, 'バ': 39, 'キ': 40, 'ペ': 41, 'ソ': 42, 'ナ': 43, 'イ': 44, 'ゼ': 45, 'ョ': 46, 'ダ': 47, 'ゴ': 48, 'ボ': 49, 'カ': 50, 'ガ': 51, 'ハ': 52, 'ベ': 53, 'コ': 54, '・': 55, 'ヌ': 56, 'オ': 57, 'ネ': 58, 'ド': 59, 'ズ': 60, 'モ': 61, 'ウ': 62, 'ジ': 63, 'ニ': 64, 'ュ': 65, 'メ': 66, 'ゲ': 67, 'ャ': 68, 'ピ': 69, 'プ': 70, 'セ': 71, 'ゾ': 72, 'チ': 73, 'ヨ': 74, 'ギ': 75, 'ヤ': 76, 'ホ': 77, 'ム': 78, 'ゥ': 79, 'ワ': 80, 'ケ': 81, ' ': 82, 'ヅ': 83, 'ヂ': 84, 'ヱ': 85, 'ヮ': 86, 'ヰ': 87, 'ヲ': 88}\n",
      "{1: 'SOS', 2: 'u', 3: 'n', 4: 's', 5: 'c', 6: 'h', 7: 'o', 8: 'l', 9: 'i', 10: 'g', 11: 'v', 12: 'e', 13: 'm', 14: 'j', 15: 'a', 16: 'b', 17: 'r', 18: ' ', 19: 'p', 20: 't', 21: 'd', 22: 'k', 23: 'y', 24: 'z', 25: 'f', 26: 'ü', 27: 'w', 28: 'x', 29: 'q', 30: 'ú', 31: 'ž', 32: 'ý', 33: 'ó', 34: 'ù', 35: 'ò', 36: '2', 37: '5', 38: '0', 39: 'ê', 40: 'ż', 41: 'õ', 42: 'þ', 43: '3', 44: 'ľ', 45: '4', 46: 'ļ', 47: 'ź', 48: '6', 49: '1', 50: '7', 51: 'ŵ', 52: '9', 53: '8'}\n"
     ]
    }
   ],
   "source": [
    "print(katakanaEncoder.char2index)\n",
    "print(englishEncoder.index2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have succesfully created our encoding based on the input data. We now have to convert the data using the encoding we created. We also have to pad both the input and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(encoder, word, word_length):\n",
    "    transformed_word = [0]*word_length\n",
    "    for i in range(len(word)):\n",
    "        transformed_word[i] = encoder.char2index[word[i]]\n",
    "    return transformed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = max(len(word) for word in X)\n",
    "OUTPUT_LENGTH = max(len(word) for word in Y)\n",
    "X = [indexesFromWord(englishEncoder, word, INPUT_LENGTH) for word in X]\n",
    "Y = [indexesFromWord(katakanaEncoder, word, OUTPUT_LENGTH) for word in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 7, 8, 9, 3, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 7, 11, 7, 4, 9, 5, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building our model. We first need to turn our input vector to a dense vector. For this, we have to use and Embedding layer. We then add one or more LSTM layers. These layers will serve as our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "encoder = Embedding(englishEncoder.n_chars, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(64, return_sequences=False)(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder model will generate a sequence of Katakanas. Every generated character will be used as the input of the decoder to generate the next one.\n",
    "The input will be passed to an Embedding layer to transform the input into dense vectors , just as we did for the encoder.\n",
    "We also need the encoder output to initialize the decoder.\n",
    "The final layer will be (time distributed) Dense layer that will produce the softmax prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n",
    "decoder = Embedding(katakanaEncoder.n_chars, 64, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(64, return_sequences=True)(decoder, initial_state=[encoder, encoder])\n",
    "decoder = TimeDistributed(Dense(katakanaEncoder.n_chars, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do in order to use this decoder model is to have a one-hot encoded structure for the softmax prediction. Also, we need to add the start sequence char at the beginning of every word in order to have our first input character for the decoder. We will also use crossvalidation to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Input Train\n",
    "encoder_input_train = X_train\n",
    "\n",
    "# Decoder Input Train\n",
    "decoder_input_train = np.zeros_like(Y_train)\n",
    "decoder_input_train[:][1:] = Y_train[:][:-1]\n",
    "decoder_input_train[:][0] = SOW_token\n",
    "\n",
    "# Decoder OutputTrain\n",
    "decoder_output_train = np.eye(katakanaEncoder.n_chars)[Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Input Validation\n",
    "encoder_input_validation = X_val\n",
    "\n",
    "# Decoder Input Validation\n",
    "decoder_input_validation = np.zeros_like(Y_val)\n",
    "decoder_input_validation[:][1:] = Y_val[:][:-1]\n",
    "decoder_input_validation[:][0] = SOW_token\n",
    "\n",
    "# Decoder Output Validation\n",
    "decoder_output_validation = np.eye(katakanaEncoder.n_chars)[Y_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! We can now train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96534 samples, validate on 10727 samples\n",
      "Epoch 1/60\n",
      "96534/96534 [==============================] - 498s 5ms/step - loss: 3.1822 - val_loss: 3.0597\n",
      "Epoch 2/60\n",
      "96534/96534 [==============================] - 495s 5ms/step - loss: 2.9172 - val_loss: 2.6732\n",
      "Epoch 3/60\n",
      "21632/96534 [=====>........................] - ETA: 8:51 - loss: 2.6405"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-b7f6e798ec00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_validation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdecoder_output_validation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                 \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                                  batch_size = 64)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "if os.path.isfile('model.h5'):\n",
    "    model = load_model('model.h5')\n",
    "else:\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit(x=[encoder_input_train, decoder_input_train], y=[decoder_output_train],\n",
    "              validation_data=([encoder_input_validation, decoder_input_validation], [decoder_output_validation]),\n",
    "                                epochs = 60,\n",
    "                                 batch_size = 64)\n",
    "   \n",
    "    \n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
