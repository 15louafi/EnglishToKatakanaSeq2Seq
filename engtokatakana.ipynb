{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Katakana seq2seq model (credit to wanasit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a .csv file, containing english words on the first column and their translation in katakana in the second column (built by wanasit, see https://wanasit.github.io/english-to-katakana-using-sequence-to-sequence-in-keras.html for more details). We're going to build a model that automatically does the conversion from english to katakana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0                1\n",
      "76132           Chewbacca           チューバッカ\n",
      "88968         Marin Bikes         マリン・バイクス\n",
      "12942             Greyout           グレイアウト\n",
      "58630   Georges Rodenbach    ジョルジュ・ローデンバック\n",
      "20202        Mike Maignan         マイク・メニャン\n",
      "44311    Jacob Ellehammer        ヤコブ・エレハマー\n",
      "90928        Samurai Jack         サムライジャック\n",
      "23014          Jony López  ジョナタン・ロペス・ロドリゲス\n",
      "44733          Ben Revere           ベン・リビア\n",
      "63059       Streptokinase        ストレプトキナーゼ\n",
      "16678              Ossian             オシアン\n",
      "92801   Gouverneur Morris        ガバヌーア・モリス\n",
      "61466           Stralsund        シュトラールズント\n",
      "102620   Friedemann Layer      フリーデマン・レイヤー\n",
      "32974      Dorothy Malone        ドロシー・マローン\n",
      "43889           Sally Yeh          サリー・イップ\n",
      "88822            Parndorf           パルンドルフ\n",
      "60547            Landshut           ランツフート\n",
      "86368      Heike Makatsch         ハイケ・マカチュ\n",
      "12315               Evraz        エブラズ・グループ\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./trainingdata/joined_titles.csv', header=None)\n",
    "print(data.sample(20, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this data into a X and Y vectors for training first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [word.lower() for word in data[0]]\n",
    "Y = [word for word in data[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not done yet. Our model only takes numerical data and we cannot input strings directly. We have to build a sort of encoding for each character in english and in katakana, but also store the way to decode the characters so we can read the output of the model at the end. Also remember that a model only takes input with same size, so we have to use padding. Let's use 0 for padding, 1 for start of sequence, and just code all the characters as int based on the order they appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "\n",
    "\n",
    "class CharEncoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.char2index = {} #the index will be our encoding of the char\n",
    "        self.char2count = {}\n",
    "        self.index2char = {1: \"SOS\"}\n",
    "        self.n_chars = 2  # Count SOS \n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishEncoder = CharEncoder(\"english\")\n",
    "katakanaEncoder = CharEncoder(\"katakana\")\n",
    "for word in X:\n",
    "    englishEncoder.addWord(word)\n",
    "for word in Y:\n",
    "    katakanaEncoder.addWord(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ア': 2, 'ン': 3, 'ス': 4, 'ク': 5, 'ー': 6, 'リ': 7, 'グ': 8, 'ロ': 9, 'ヴ': 10, 'ォ': 11, 'シ': 12, 'ツ': 13, 'ェ': 14, 'ミ': 15, 'ル': 16, 'ヒ': 17, 'ユ': 18, 'ァ': 19, 'ブ': 20, 'レ': 21, 'ビ': 22, 'ッ': 23, 'ラ': 24, 'サ': 25, 'パ': 26, 'マ': 27, 'ノ': 28, 'ザ': 29, 'ポ': 30, 'ト': 31, 'デ': 32, 'フ': 33, 'テ': 34, 'エ': 35, 'ヘ': 36, 'タ': 37, 'ィ': 38, 'バ': 39, 'キ': 40, 'ペ': 41, 'ソ': 42, 'ナ': 43, 'イ': 44, 'ゼ': 45, 'ョ': 46, 'ダ': 47, 'ゴ': 48, 'ボ': 49, 'カ': 50, 'ガ': 51, 'ハ': 52, 'ベ': 53, 'コ': 54, '・': 55, 'ヌ': 56, 'オ': 57, 'ネ': 58, 'ド': 59, 'ズ': 60, 'モ': 61, 'ウ': 62, 'ジ': 63, 'ニ': 64, 'ュ': 65, 'メ': 66, 'ゲ': 67, 'ャ': 68, 'ピ': 69, 'プ': 70, 'セ': 71, 'ゾ': 72, 'チ': 73, 'ヨ': 74, 'ギ': 75, 'ヤ': 76, 'ホ': 77, 'ム': 78, 'ゥ': 79, 'ワ': 80, 'ケ': 81, ' ': 82, 'ヅ': 83, 'ヂ': 84, 'ヱ': 85, 'ヮ': 86, 'ヰ': 87, 'ヲ': 88}\n",
      "{1: 'SOS', 2: 'u', 3: 'n', 4: 's', 5: 'c', 6: 'h', 7: 'o', 8: 'l', 9: 'i', 10: 'g', 11: 'v', 12: 'e', 13: 'm', 14: 'j', 15: 'a', 16: 'b', 17: 'r', 18: ' ', 19: 'p', 20: 't', 21: 'd', 22: 'k', 23: 'y', 24: 'z', 25: 'f', 26: 'ü', 27: 'w', 28: 'x', 29: 'q', 30: 'ú', 31: 'ž', 32: 'ý', 33: 'ó', 34: 'ù', 35: 'ò', 36: '2', 37: '5', 38: '0', 39: 'ê', 40: 'ż', 41: 'õ', 42: 'þ', 43: '3', 44: 'ľ', 45: '4', 46: 'ļ', 47: 'ź', 48: '6', 49: '1', 50: '7', 51: 'ŵ', 52: '9', 53: '8'}\n"
     ]
    }
   ],
   "source": [
    "print(katakanaEncoder.char2index)\n",
    "print(englishEncoder.index2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have succesfully created our encoding based on the input data. We now have to convert the data using the encoding we created. We also have to pad both the input and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(encoder, word, word_length):\n",
    "    transformed_word = [0]*word_length\n",
    "    for i in range(len(word)):\n",
    "        transformed_word[i] = encoder.char2index[word[i]]\n",
    "    return transformed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = max(len(word) for word in X)\n",
    "OUTPUT_LENGTH = max(len(word) for word in Y)\n",
    "X = [indexesFromWord(englishEncoder, word, INPUT_LENGTH) for word in X]\n",
    "Y = [indexesFromWord(katakanaEncoder, word, OUTPUT_LENGTH) for word in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5, 6, 7, 7, 8, 9, 3, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 7, 11, 7, 4, 9, 5, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start building our model. We first need to turn our input vector to a dense vector. For this, we have to use and Embedding layer. We then add one or more LSTM layers. These layers will serve as our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "encoder = Embedding(englishEncoder.n_chars, 64, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(64, return_sequences=True)(encoder)\n",
    "encoder = LSTM(64, return_sequences=False)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
